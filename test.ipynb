{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed_data_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 0':\"case_id\"},inplace=True)\n",
    "df.set_index(df['case_id'],inplace=True)\n",
    "df.drop(columns=[\"case_id\"],inplace=True)\n",
    "print(\"CASE_IDs are set as index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter_id = []\n",
    "for case_id in df.index:\n",
    "    submitter_id.append(case_id[:12])\n",
    "submitter_ids = set(submitter_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One submitter id can have multiple samples but all the samples belongs to same primary site\n",
    "### One TCGA project can have tumor data of multiple primary sites\n",
    "### Primary site examples : [kindney,Lungs,Brain,Breast,etc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_tumor_site_data(transposed_rna_seq_df,starting_index,stopping_index,backup_data):\n",
    "    print(f\"Fetching data from {starting_index} to {stopping_index}\")\n",
    "    output_list = []\n",
    "    case_id_list = []\n",
    "    cases_endpt = 'https://api.gdc.cancer.gov/cases'\n",
    "    fields = [\n",
    "        \"primary_site\",\n",
    "        ]\n",
    "    fields = ','.join(fields)\n",
    "    for i,case_id in enumerate(transposed_rna_seq_df.index):\n",
    "        if i< starting_index:\n",
    "            continue\n",
    "        if i > stopping_index:\n",
    "            break\n",
    "        print(\"==\"*50)\n",
    "        print(f\"Data querying for case_id {i}\")\n",
    "        case_id_list.append(case_id)\n",
    "        filters = {\n",
    "            \"op\": \"in\",\n",
    "            \"content\":{\n",
    "                \"field\": \"submitter_id\",\n",
    "                \"value\":case_id[:12]\n",
    "                }\n",
    "            }\n",
    "\n",
    "        params = {\n",
    "            \"filters\": json.dumps(filters),\n",
    "            \"fields\": fields,\n",
    "            \"format\": \"JSON\",\n",
    "            \"size\": \"100\"\n",
    "            }\n",
    "\n",
    "        response = requests.get(cases_endpt, params = params)\n",
    "        response_json = response.json()\n",
    "        hits = response_json.get(\"data\", {}).get(\"hits\", [])\n",
    "        if not hits:\n",
    "            print(f\"No data returned for case_id: {case_id}\")\n",
    "            continue  # Skip if no hit is returned\n",
    "\n",
    "        case_data = hits[0]\n",
    "        primary_site = case_data.get(\"primary_site\", \"\")\n",
    "        primary_site = primary_site.replace(\" \", \"_\")\n",
    "        \n",
    "        output_data = {\n",
    "            \"case_id\": case_id,\n",
    "            \"uuid\": case_data.get(\"id\", \"\"),\n",
    "            \"tumour_site\": primary_site\n",
    "        }\n",
    "        print(f\"Data found for case_id {i}\")\n",
    "        print(\"Going to sleep\")\n",
    "        output_list.append(output_data)\n",
    "        backup_data.append(output_data)\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10471"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processed : 2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sample_tumor_site_data(df,5099,6000,backup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5097"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5000\n",
    "e = 6000\n",
    "output_dataframe = pd.DataFrame(backup_data[s:])\n",
    "output_dataframe.set_index(\"case_id\", inplace=True)\n",
    "output_dataframe.to_csv(f\"data/label/sample_tumourtype_map_{s}_{e}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(\"/Users/abir/Developer/pancan_project/data/label/sample_tumourtype_map_0_199.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def combine_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Read all CSV files in the specified folder and append them row-wise into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined DataFrame with all data from CSV files\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    # Check if any CSV files were found\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return combined_df\n",
    "    \n",
    "    # Loop through each CSV file and append to the combined DataFrame\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            # Read the current CSV file\n",
    "            current_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Append to the combined DataFrame\n",
    "            combined_df = pd.concat([combined_df, current_df], ignore_index=True)\n",
    "            \n",
    "            print(f\"Successfully read {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    print(f\"Combined {len(csv_files)} CSV files into a DataFrame with {combined_df.shape[0]} rows and {combined_df.shape[1]} columns.\")\n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read sample_tumourtype_map_1200_1399.csv\n",
      "Successfully read sample_tumourtype_map_600_799.csv\n",
      "Successfully read sample_tumourtype_map_2500_2999.csv\n",
      "Successfully read sample_tumourtype_map_0_199.csv\n",
      "Successfully read sample_tumourtype_map_3000_3999.csv\n",
      "Successfully read sample_tumourtype_map_1000_1199.csv\n",
      "Successfully read sample_tumourtype_map_800_999.csv\n",
      "Successfully read sample_tumourtype_map_1800_1999.csv\n",
      "Successfully read sample_tumourtype_map_2200_2499.csv\n",
      "Successfully read sample_tumourtype_map_1600_1799.csv\n",
      "Successfully read sample_tumourtype_map_2121_2199.csv\n",
      "Successfully read sample_tumourtype_map_4000_4198.csv\n",
      "Successfully read sample_tumourtype_map_4198_5000.csv\n",
      "Successfully read sample_tumourtype_map_2000_2199.csv\n",
      "Successfully read sample_tumourtype_map_1400_1599.csv\n",
      "Successfully read sample_tumourtype_map_400_599.csv\n",
      "Successfully read sample_tumourtype_map_200_399.csv\n",
      "Successfully read sample_tumourtype_map_1894_1999.csv\n",
      "Successfully read sample_tumourtype_map_1307_1399.csv\n",
      "Combined 19 CSV files into a DataFrame with 5268 rows and 3 columns.\n"
     ]
    }
   ],
   "source": [
    "combined_data = combine_csv_files(\"/Users/abir/Developer/pancan_project/data/label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5268, 3)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.to_pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save DataFrame to pickle file\n",
    "def save_dataframe_to_pickle(df, file_path):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to a pickle file\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame to save\n",
    "        file_path (str): Path where the pickle file will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_pickle(file_path)\n",
    "        print(f\"DataFrame successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame: {e}\")\n",
    "\n",
    "# Read DataFrame from pickle file\n",
    "def read_dataframe_from_pickle(file_path):\n",
    "    \"\"\"\n",
    "    Read a pandas DataFrame from a pickle file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the pickle file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame loaded from the pickle file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "        print(f\"DataFrame successfully loaded from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DataFrame: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to data/processed_data/label_data.pkl\n",
      "DataFrame successfully saved to data/processed_data/gene_exp_data.pkl\n"
     ]
    }
   ],
   "source": [
    "save_dataframe_to_pickle(df=combined_data,file_path=\"data/processed_data/label_data.pkl\")\n",
    "save_dataframe_to_pickle(df=df.iloc[:5268],file_path=\"data/processed_data/gene_exp_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pancan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
