{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b1f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from utils import *\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,precision_score,recall_score\n",
    "from scipy.stats import uniform, randint\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e6d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully loaded from data/processed_data/gene_exp_data.pkl\n",
      "DataFrame successfully loaded from data/processed_data/label_data.pkl\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Entries in Gene Expression Dataframe : 5268\n",
      "Entries in Label Dataframe : 5268\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Entries in Extracted Gene Expression Dataframe : 4392\n",
      "Entries in Extracted Label Dataframe : 4392\n"
     ]
    }
   ],
   "source": [
    "gene_exp_df = read_dataframe_from_pickle(\"data/processed_data/gene_exp_data.pkl\")\n",
    "label_df = read_dataframe_from_pickle(\"data/processed_data/label_data.pkl\")\n",
    "print(\"--\" * 80)\n",
    "print(f\"Entries in Gene Expression Dataframe : {len(gene_exp_df)}\")\n",
    "print(f\"Entries in Label Dataframe : {len(label_df)}\")\n",
    "labels_with_high_freq_df = remove_low_frequency_labels(label_df,threshold=150)\n",
    "extracted_data,extracted_label = collect_relevant_data(gene_exp_df_bkp=gene_exp_df,label_df_bkp=labels_with_high_freq_df)\n",
    "encoded_labels,label_encoder = encode_labels(extracted_label)\n",
    "print(\"--\" * 80)\n",
    "print(f\"Entries in Extracted Gene Expression Dataframe : {len(extracted_data)}\")\n",
    "print(f\"Entries in Extracted Label Dataframe : {len(encoded_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80138ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6596ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    extracted_data, encoded_labels, \n",
    "    test_size=0.2, \n",
    "    stratify=encoded_labels,  # Critical for imbalanced data\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf74f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0c52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(model):\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecf0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logreg': LogisticRegression (multi_class='multinomial',max_iter=5000,random_state=random_state),\n",
    "    'svm': SVC(probability=True,random_state=random_state),\n",
    "    'rf': RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
    "    'lgbm': LGBMClassifier(\n",
    "                        objective='multiclass', \n",
    "                        num_class=n_classes,\n",
    "                        random_state=random_state,\n",
    "                        early_stopping_rounds=10,\n",
    "                        eval_set=[(X_test, y_test)],\n",
    "    ),         \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17215b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'logreg': {\n",
    "        'clf__C': uniform(0.01, 10),\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs'],\n",
    "        'clf__class_weight': ['balanced']\n",
    "    },\n",
    "    'svm': {\n",
    "        'clf__C': uniform(0.01, 10),\n",
    "        'clf__gamma': ['scale', 'auto'],\n",
    "        'clf__kernel': ['rbf'],\n",
    "        'clf__class_weight': ['balanced']\n",
    "    },\n",
    "    'lgbm': {\n",
    "        'clf__n_estimators': randint(100, 300),\n",
    "        'clf__max_depth': randint(3, 10),\n",
    "        'clf__learning_rate': uniform(0.01, 0.3),\n",
    "        'clf__class_weight': ['balanced'],\n",
    "        'clf__subsample': uniform(0.7, 0.3),\n",
    "        'clf__colsample_bytree': uniform(0.7, 0.3),\n",
    "    },\n",
    "    'rf': {\n",
    "        'clf__n_estimators': randint(100, 500),\n",
    "        'clf__max_depth': randint(5, 30),\n",
    "        'clf__min_samples_split': randint(2, 20),\n",
    "        'clf__min_samples_leaf': randint(1, 10),\n",
    "        'clf__class_weight': ['balanced', 'balanced_subsample']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc66ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    try:\n",
    "        print(\"==\"*40)\n",
    "        print(f\"\\nðŸ”Ž Tuning {name.upper()}\")\n",
    "        pipe = build_pipeline(model)\n",
    "        fit_params = {}\n",
    "        if name in ['lgbm']:\n",
    "            fit_params = {\n",
    "                'clf__early_stopping_rounds': 10,\n",
    "                'clf__eval_set': [(X_test, y_test)],\n",
    "            }\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_grids[name],\n",
    "            n_iter=20,\n",
    "            scoring='f1_macro',\n",
    "            n_jobs=-1,\n",
    "            cv=cv,\n",
    "            verbose=1,\n",
    "            random_state=random_state,\n",
    "            return_train_score=True \n",
    "        )\n",
    "        \n",
    "        search.fit(X_train, y_train, **fit_params)\n",
    "        print(\"--\"*35)\n",
    "        print(f\"\\nâœ… Best params for {name.upper()}:\")\n",
    "        print(search.best_params_)\n",
    "        print(f\"\\nâœ… Best score for {name.upper()}:\")\n",
    "        print(search.best_score_)\n",
    "        \n",
    "        # Store the best estimator\n",
    "        best_model = search.best_estimator_\n",
    "        best_index = search.best_index_\n",
    "        train_score = search.cv_results_['mean_train_score'][best_index]\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_test)        \n",
    "        # Evaluation metrics\n",
    "        # class_report = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "\n",
    "        results[name] = {\n",
    "            'best_params': search.best_params_,\n",
    "            \"best_test_score\": search.best_score_,\n",
    "            'best_train_score':train_score,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'test_f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "            \"precision_weighted\" : precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall_weighted\" : recall_score(y_test, y_pred, average='weighted'),\n",
    "            'best_estimator': best_model,\n",
    "        }\n",
    "        print(f\"Successfully Completed all cross validations for model : {name.upper()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered error at {name.upper} :: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55263d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Model Performance Summary:\n",
      "        model_test_score  model_train_score  test_f1_Score(weighted)  \\\n",
      "logreg           0.88564                1.0                  0.92818   \n",
      "\n",
      "        test_precision_Score(weighted)  test_recall_Score(weighted)  \\\n",
      "logreg                         0.93311                      0.92833   \n",
      "\n",
      "        test_balanced_accuracy_score  \n",
      "logreg                       0.89742  \n"
     ]
    }
   ],
   "source": [
    "# Create a summary dataframe for easy comparison\n",
    "summary = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'model_test_score': round(results[model_name][\"best_test_score\"],5),\n",
    "        'model_train_score': round(results[model_name][\"best_train_score\"],5),\n",
    "        'test_f1_Score(weighted)': round(results[model_name]['test_f1_weighted'],5),\n",
    "        'test_precision_Score(weighted)': round(results[model_name]['precision_weighted'],5),\n",
    "        'test_recall_Score(weighted)': round(results[model_name]['recall_weighted'],5),\n",
    "        'test_balanced_accuracy_score': round(results[model_name]['test_balanced_accuracy'],5),\n",
    "    } for model_name in results\n",
    "}).T\n",
    "summary.to_csv(\"results/Summary_Metrics_raw.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1caaf",
   "metadata": {},
   "source": [
    "## Model Performance Summary : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe899ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_test_score</th>\n",
       "      <th>model_train_score</th>\n",
       "      <th>test_f1_Score(weighted)</th>\n",
       "      <th>test_precision_Score(weighted)</th>\n",
       "      <th>test_recall_Score(weighted)</th>\n",
       "      <th>test_balanced_accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.88564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.92818</td>\n",
       "      <td>0.93311</td>\n",
       "      <td>0.92833</td>\n",
       "      <td>0.89742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_test_score  model_train_score  test_f1_Score(weighted)  \\\n",
       "logreg           0.88564                1.0                  0.92818   \n",
       "\n",
       "        test_precision_Score(weighted)  test_recall_Score(weighted)  \\\n",
       "logreg                         0.93311                      0.92833   \n",
       "\n",
       "        test_balanced_accuracy_score  \n",
       "logreg                       0.89742  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values('model_test_score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pancan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
